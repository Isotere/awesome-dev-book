---
tags:
  - GoLang/Scheduler/Go2
aliases:
---
# Планировщик Go

Веб-сервисы подразумевают подавляющее кол-во I/O-bound нагрузки. Довольно редко нам нужны сложные и долгие вычисления на процессоре. И в связи с этой особенностью у нас очень много частых переключений контекста (пока ждем I/O - базы данных, сетевых запросов и тд).

Использовать потоки (threads), которые предоставляет ОС, очень дорого. Объем памяти, выделяемой потоку в линуксе доходит до 8Мб. Также переключение потоков происходит очень медленно - полная перезагрузка памяти, системные вызовы в ядро ОС. И еще немаловажный фактор - легко "положить" систему: насоздавать большое кол-во потоков, которые просто "съедят" все ресурсы ОС. Использование thread-pool решает эту проблему очень слабо: у нас не может быть пул бесконечного размера и пользователи все равно будут ожидать своей очереди. (немного про это есть [здесь](awesome-dev-book/book/Язык%20Go/Scheduler/Планировщик%20ОС.md). 

![](sche_01.png)

**Контекст потока** - это в основном регистры и стек. 

Мы можем сами сделать контекст и хранить его в рамках приложения. С помощью ассемблерных инструкций мы можем "снять" значения регистров процессора, и вместе со стеком хранить в блоке памяти. 

Для сравнения: 

![](sche_02.png)

Т.е. в наших, "руко-творных" потоках стек будет от 2Кб (вместо 8Мб). К тому же он будет динамическим, т.е. может расти в зависимости от потребностей. 

Переключать наш "легковесный" контекст мы будем внутри нашего юзер спейса, внутри одного потока ОС. Планировщик ОС про это знать ничего не будет. 

## Основы

Ключевая идея: 
- Не даем в руки программисту поток ОС
- Делаем вид, что есть только горутины
- Всю сложность распределения горутин - абстрагируем

В самом простом виде можно было бы (можно было бы) создать очередь из горутин привязанную к потоку нашей программы и планировщик бы в каком-то порядке брал из очереди (FIFO) горутину, исполнял, клал обратно в хвост.

### Тип многозадачности

В отличие от ОС, где реализована вытесняющая многозадачность, в Го планировщике реализована **кооперативная многозадачность**. Компилятор на этапе компиляции в некоторых местах добавляет команды переключения контекста (например в момент проверки необходимого пространства для стека горутины, когда вызывается какая-либо функция). Так же разработчик сам может вызвать планировщик из кода. 

### Масштабирование 

> Как быть с много-ядерными процессорами? 


Как решение - создавать M потоков сразу, которые будут обслуживать N горутин (так называемая **N-M модель**). Но может возникнуть проблема. Если у нас все горутины будут в одной очереди, и каждый поток будет обращаться к ней за следующей горутиной, то необходимо (например с помощью мютекса) блокировать каждый раз очередь. Мы получаем узкое место. 

Как выход - "шардировать" эту глобальную очередь! И мы получим следующее:

![](sche_03.png)


### Локальная очередь

Те за каждым потоком закрепляется его личная, локальная очередь. И таким образом мы снимаем проблему синхронизации. В каждой локальной очереди может быть не более 256 горутин.

Также вводим понятие "виртуальный процессор" (Р). Те это такая абстракция, у которой есть свои ресурсы для выполнения процедуры (очередь горутин). 

Получаем: 

![](sche_04.png)

Связь M -> P - один к одному. 

Теперь возникает вопрос: в какую из очередей добавлять новую горутину? Будем добавлять в одну из локальных очередей. Возникает следующий вопрос: что делать, если в очереди закончатся горутины? 

Приходим к... 

### Work Stealing

(размер локальной очереди ограничен - 256 штук. Почему. Хз. Вариации на размер байта мб)

Те потоки, у которых очередь пуста, они будут ходить в другую локальную очередь и брать оттуда горутины. Но опять всплывает проблема синхронизации. И тут уже от этого не уйти. Нужно выбрать - какой примитив синхронизации выбрать. Остановим выбор на lock-free синхронизации (атомики и CAS-loop) - просто потому, что они быстрее. 

А какой именно очереди красть? Чтобы не думать долго - выбираем рендомно. 

А что если и та очередь пуста? Ретраим. 4 раза (почему? хз). 

Сколько же нам взять из другой очереди? Заберем половину. Почему? Да чтобы реже ходить и мешать другим потокам. 

### Syscalls

I/O-bound нагрузка - это постоянные syscall. Syscall - это прыжок в ядро: те пользовательский поток (в которой крутится горутина данная) - он блокируется. Компилятор (рантайм) знает, когда планируется сискол. Он оборачивает его и перед его вызовом - открепляет поток и "кладем" его рядом, пусть он ждет ответа I/O. 

А чтобы оставшаяся локальная очередь "не голодала" - мы прикрепляем к ней новый поток. Для того, чтобы каждый раз не создавать новые потоки (медленно), создается thread-pool, и те открепленные потоки, которые "отработали" - остаются там. И когда локальной очереди нужно будет сделать новый поток - он возьмется из pool. 

Но не все I/O операции долгоиграющие. Есть short-live операции. В таких случаях - поток не будет открепляться. 

Но что делать если все же блокировка будет "долгой"? (ОС может быть занята и не возьмет долгое время в работу поток). Для этого создается отдельный, дополнительный поток (sysmon), который мониторит эти ситуации. И если поток долгое время idle - он его открепляет. 

![](sche_05.png)

Следующий вопрос. Куда девать горутину после syscall? 

Будем добавлять горутину в очередь того процессора, где она была. (мы можем сохранить в контексте горутины эту информацию). Если этот процессор (его очередь) заполнен либо выполняет syscall, то ищем другой процессор. 

### Глобальная очередь

Если не нашли такого процессора - то добавляем горутину в глобальную очередь. (заводим новую абстракцию). Для глобальной очереди тоже нужна синхронизация, но уже будем использовать мютекс. 

После всего, что мы узнали, появляется вопрос: **а что делать если у нас 10000 потоков уйдут в I/O в пулл ожидания**? (при размере потока 8Мб - теряем сразу 80Гб оперативы)

Для того, чтобы горутины в глобальной очеред не "голодали" и дождались своего звездного часа: 

![](sche_06.png)

Каждый 61 тик планировщика (переключение) - проверяем глобальную очередь, если нет чекаем локальную, если тоже нет - пытаемся спереть у соседа, если и у него нет идем в netpoller.

### Мультиплексирование (netpoll)

В линуксе это epoll, в маке - k-queue

Этот инструмент просто хранит все эти дескрипторы (сокеты) ожидающие read-write операции и с помощью event-loop периодически опрашивает о готовности. 

Т.е. мы вместо того чтобы откреплять поток и класть его в pull, просто отдаем netpoller этот дескриптор и он опрашивает. По готовности он уведомляет соответстующую горутину и мы ее по предыдущему сценарию возвращаем в очередь. 

> NetPoller используется при ассинхронном вызове, отсоединение потока - при блокирующем чтении (доступ к ФС)

В картинках можно посмотреть [здесь](Планировщик%20Go.md)

### Общение по каналам и кеш процессора

Горутина использует кеш процессора. При общении с какими то общими данными (каналами) при ожидании доступа (к данным, каналу) она будет вытеснена планировщикам. Если использовать очередь FIFO то при большом кол-ве горутин, пока до нее дойдет очередь - кеш протухнет. Можно использовать очередь LIFO, но тогда есть шанс, что с головы очереди долго не будет читаться что-то. Для этого придумали комбинированную очередь: 

![](sche_07.png)

Те горутина будет помещаться в очередь LIFO (одноместную). Если там была другая горутина - она будет вытеснена в FIFO. Для того, чтобы не получилось так, что зацикленные горутины (общающиеся между собой по каналам - одна читает, другая пишет, потом первая пишет вторая читает) не крутились между собой - на поток sysmon повесили обязанность следить за этой очередью и горутиной которая там крутится. Если она находится там больше определенного времени - вытеснять ее в FIFO. 

> этот момент я понимаю где-то на грани, надо бы проработать его


### Синхронизация

Примитивы синхронизации ОС нам не подходят - они будут блокировать выполнение потока, а мы этого пытаемся избежать. 

Сделаем свой "лунапарк с блекджеком и тд". 

Написали свой мютекс на атомиках. Планировщик при его вызове все видит, все знает и вытесняет такую горутину. Но перед тем как это сделать - прокрутит несколько раз CAS-loop и если не сработало, то только в этом случае вытеснит. 

### Состояние горутины 

- **Выполняется** - running
- **Готова к выполнению** - runnable
- **Остановлена и ожидает чего-то** - waiting

Для последних делаем отдельную очередь - **Wait Queue**

![](sche_08.png)

В чем смысл этой картинки... Может получиться режим голодания, когда горутины в очереди ожидания никак не дождутся своей очереди (мютекс разблокировался, но его сразу заняла "живая" горутина). Поэтому при разблокировке мютекса проверяется очередь ожидания и если там есть горутина которая ждет его больше какого-то времени - то лок отдается ей, а "живая" горутина ставится в очередь ожидания. 


### Циклы

Выше мы исходили из предположения что у нас основная нагрузка это I/O bound нагрузка. Но все же могут быть задачи, где преобладает CPU-Bound нагрузка (сложные вычислительные задачи) либо бесконечные циклы. 

Для таких случаев добавили **ассинхронную вытесняющую многозадачность**. Т.е. sysmon проверяет запущенные горутины и как только обнаруживают такую, которая выполняется более 10мс - посылает сигнал на ее вытеснение. Горутина может быть вытеснена не сразу - она должна в своем выполнении дойти до какого-то безопасного места и там она вылетит (небезопасные участки - unsafe участки кода например). 

